# -*- coding: utf-8 -*-
"""Proyecto_Final_ M7_Elias_Aburto

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZQM5i1fPIjkUluYRtVMZPlSM46k9Kzc3

## **Bootcamp: Ciencia de Datos e Inteligencia Artificial**
## **Proyecto del Módulo 7: Técnicas avanzadas para ciencia de datos y empleabilidad**

Hola, ya es el último proyecto, has avanzado y aprendido mucho hasta acá. ¡Muchas felicidades!

Es hora de poner en práctica todo lo que hemos aprendido a lo largo de nuestra travesía.

Lee el proyecto y revisa con cuidado cada una de las instrucciones. Procura plasmar todo tu potencial para que lo concluyas de manera sobresaliente.

¡Éxito!

# Objetivos
- Aplicar con éxito todos los conocimientos que has adquirido a lo largo del Bootcamp.
- Consolidar las técnicas de limpieza, entrenamiento, graficación y ajuste a modelos de *Machine Learning*.
- Generar una API que brinde predicciones como resultado a partir de datos enviados.

# Proyecto

1. Selecciona uno de los siguientes *datasets*:
  - Imágenes de rayos X de pecho para detectar neumonía: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia
  - *Reviews* de aplicaciones de la Google Play Store: https://www.kaggle.com/datasets/lava18/google-play-store-apps
  - Estadísticas demográficas de los ganadores del premio Oscar de la Academia: https://www.kaggle.com/datasets/fmejia21/demographics-of-academy-awards-oscars-winners
  - Aspiraciones profesionales de la generación Z: https://www.kaggle.com/datasets/kulturehire/understanding-career-aspirations-of-genz

Cada uno representa un *dataset*, un problema y una forma diferente de abordarlo. Tu tarea es identificar las técnicas y modelos que podrías usar para tu proyecto.

2. Debes hacer un análisis exploratorio y limpieza de los datos. Usa las ténicas que creas convenientes.

3. Entrena el modelo de *Machine Learning*, procesamiento de lenguaje natural o red neuronal que creas adecuado.

4. Genera por lo menos dos gráficas y dos métricas de rendimiento; explica las puntuaciones de rendimiento que amerite tu problema. Todas las gráficas de rendimiento que realices deben tener leyendas, colores y títulos personalizados por ti.

  - Además, antes de subir el modelo a "producción", deberás realizar un proceso de ensambles (*ensemblings*) y de ajuste de hiperparámetros o *tuning* para intentar mejorar la precisión y disminuir la varianza de tu modelo.

5. Construye una API REST en la que cualquier usuario pueda mandar datos y que esta misma devuelva la predicción del modelo que has hecho. La API debe estar en la nube, ya sea en un servicio como Netlify o Ngrok, para que pueda ser consultada desde internet.

6. Genera una presentación del problema y del modelo de solución que planteas. Muestra gráficas, datos de rendimiento y explicaciones. Esta presentación debe estar enfocada a personas que no sepan mucho de ciencia de datos e inteligencia artificial.

7. **Solamente se recibirán trabajos subidos a tu cuenta de GitHub con un README.md apropiado que explique tu proyecto**.

## Criterios de evaluación

| Actividad | Porcentaje | Observaciones | Punto parcial
| -- | -- | -- | -- |
| Actividad 1. Limpieza y EDA | 20 | Realiza todas las tareas necesarias para hacer el EDA y la limpieza correcta, dependiendo de la problemática. Debes hacer como mínimo el análisis de completitud, escalamiento (si aplica) y tokenización (si aplica). | Realizaste solo algunas tareas de exploración y limpieza y el modelo se muestra aún con oportunidad de completitud, escalamiento y/o mejora. |
| Actividad 2. Entrenamiento del modelo | 20 | Elige el modelo y algoritmo adecuados para tu problema, entrénalo con los datos ya limpios y genera algunas predicciones de prueba. | No has realizado predicciones de prueba para tu modelo de ML y/o tu modelo muestra una precisión menor al 60 %. |
| Actividad 3. Graficación y métricas | 20 | Genera por lo menos dos gráficas y dos muestras de métricas que permitan visualizar el rendimiento y precisión del modelo que construiste. Además, realizaste los procesos de *tuning* y ensambles adecuados para tu problema. | Las gráficas no tienen leyendas y colores customizados, solo muestras una gráfica o no realizaste el *tuning* de hiperparámetros.
| Actividad 4. API REST | 20 | Generaste con éxito un *link* público en el que, por método POST, se puede mandar información y la API REST devuelve una predicción junto con el porcentaje de confianza de esta misma. | N/A
| Actividad 5. Presentación | 20 | Genera una presentación en la que establezcas como mínimo: el problema, proceso de solución, metodologías usadas, gráficas de rendimiento, demostración del modelo y aprendizajes obtenidos. Debes redactarla con términos que pueda entender cualquier persona, no solo científicos de datos. | La presentación no expone con claridad o en términos coloquiales el proceso de creación del modelo, sus ventajas y muestras de rendimiento.

**Mucho éxito en tu camino como Data Scientist.**

# Definicion de Target

Seleccionamos como variable objetivo (target) la columna “What is the most preferred working environment for you.” porque representa una preferencia laboral concreta y directamente interpretable, lo que facilita comunicar el valor del modelo a un público no técnico. Además, es un problema de clasificación multiclase con un número manejable de categorías (6), lo que permite entrenar y comparar distintos modelos (incluyendo ensambles) sin que el espacio de clases sea excesivo para el tamaño del dataset (235 registros). Este target también habilita una evaluación sólida y alineada con la rúbrica del módulo, ya que permite reportar métricas apropiadas para multiclase (por ejemplo, accuracy y F1-score) y visualizar el desempeño con gráficos como la matriz de confusión. Finalmente, esta elección se adapta bien al requerimiento de una API con “confianza”, ya que los modelos de clasificación pueden entregar probabilidades por clase, interpretables como un nivel de confianza asociado a la predicción.
"""

# PASO 1: Montar Google Drive y cargar el dataset
from google.colab import drive
import pandas as pd

RANDOM_STATE = 42

drive.mount("/content/drive")

DATA_PATH = "/content/drive/MyDrive/Datasets Bootcamp UDD/Your Career Aspirations of GenZ.csv"
df = pd.read_csv(DATA_PATH)

df.shape

"""Montamos Google Drive para acceder al archivo CSV desde una ruta estable. Definimos RANDOM_STATE=42 para que todo el proyecto sea reproducible. Luego cargamos el dataset con pandas.read_csv() y mostramos df.shape para confirmar filas y columnas."""

# Mostrar las primeras 3 filas
df.head(3)

# Mostrar la lista de columnas (para validar nombres exactos)
df.columns.tolist()

# PASO 2: Auditoria rapida de calidad (tipos, nulos, duplicados)
import pandas as pd

# 1) Tipos de datos
dtypes_table = df.dtypes.value_counts()
dtypes_table

"""Contamos los tipos de datos presentes para confirmar que la mayoría son categóricos (object) y detectar variables numéricas que podrían requerir tratamiento especial."""

# 2) Porcentaje de nulos por columna (ordenado)
null_pct = (df.isna().mean() * 100).sort_values(ascending=False)
null_pct

"""Calculamos el porcentaje de valores nulos por columna para evaluar completitud y decidir si se imputará, se eliminarán filas/columnas o si no es necesario intervenir."""

# 3) Duplicados exactos
df.duplicated().sum()

"""Medimos cuántas filas son duplicados exactos para decidir si corresponde eliminarlos antes del modelado."""

# 4) Cardinalidad por columna categorica (para evaluar rare->OTHER)
cat_cols_all = df.select_dtypes(include=["object"]).columns
cardinality = df[cat_cols_all].nunique().sort_values(ascending=False)
cardinality

"""Calculamos la cantidad de categorías únicas por variable categórica para anticipar columnas de alta cardinalidad y justificar el agrupamiento de categorías raras en "OTHER".

Lo que ya sabemos (y por qué importa)

Tipos: 13 columnas object (categóricas) y 2 int64 (numéricas).

Nulos: 0% en todas las columnas → no necesitamos imputación.

Duplicados: 0 → no necesitamos deduplicar.

Cardinalidad:

"Which of the below careers looks close to your Aspirational job ?" tiene 84 categorías → confirma que es una buena candidata a DROP_COLS (alta cardinalidad).

"Your Current Zip Code / Pin Code" no aparece en cardinalidad porque es int64, pero funciona como identificador (alta cardinalidad esperable) → también candidata a DROP_COLS.
"""

# PASO 3: Distribucion del target original (6 categorias)
TARGET_6 = "What is the most preferred working environment for you."

target_counts = df[TARGET_6].value_counts(dropna=False)
target_counts

"""Mostramos el conteo de cada categoría del target original para confirmar las clases reales presentes en el dataset y su distribución."""

# PASO 3: Distribucion del target original (6 categorias)
TARGET_6 = "What is the most preferred working environment for you."

target_counts = df[TARGET_6].value_counts(dropna=False)
target_counts

"""Mostramos la distribución del target original con sus 6 categorías para entender cómo se compone la variable objetivo antes de binarizarla."""

# PASO 3: Crear target binario (FULL_OFFICE vs REMOTE)
df["target_bin"] = df[TARGET_6].astype(str).apply(
    lambda x: "FULL_OFFICE" if x.strip() == "Every Day Office Environment" else "REMOTE"
)

bin_counts = df["target_bin"].value_counts()
bin_pct = (bin_counts / bin_counts.sum() * 100).round(2)

pd.DataFrame({"count": bin_counts, "pct": bin_pct})

"""Transformamos el target original en un target binario. Se etiqueta como FULL_OFFICE cuando la preferencia es “Every Day Office Environment” (100% oficina). Todas las demás preferencias se etiquetan como REMOTE (incluye trabajo remoto parcial o total). Luego calculamos conteos y porcentajes para validar el balance."""

# PASO 3: Validacion del mapeo (categorias originales dentro de cada clase)
remote_categories = df.loc[df["target_bin"] == "REMOTE", TARGET_6].value_counts()
full_office_categories = df.loc[df["target_bin"] == "FULL_OFFICE", TARGET_6].value_counts()

full_office_categories, remote_categories

"""Verificamos qué categorías originales del target se agrupan en FULL_OFFICE y cuáles en REMOTE, dejando el mapeo explícito y verificable."""

import matplotlib.pyplot as plt
import pandas as pd

TARGET_6 = "What is the most preferred working environment for you."

# Asegurar target_bin con la nueva logica
if "target_bin" not in df.columns:
    df["target_bin"] = df[TARGET_6].astype(str).apply(
        lambda x: "FULL_OFFICE" if x.strip() == "Every Day Office Environment" else "REMOTE"
    )

counts = df["target_bin"].value_counts()
pct = (counts / counts.sum() * 100).round(2)

plt.figure()
ax = counts.plot(kind="bar")
plt.title("Distribucion del target binario: conteo y porcentaje")
plt.xlabel("Clase")
plt.ylabel("Cantidad")
plt.xticks(rotation=0)

for i, (label, n) in enumerate(counts.items()):
    ax.text(i, n, f"{n} ({pct[label]}%)", ha="center", va="bottom")

plt.tight_layout()
plt.show()

pd.DataFrame({"count": counts, "pct": pct})

"""Mostramos la distribución del target binario con barras (conteo) y anotamos arriba de cada barra el porcentaje correspondiente, para comunicar tamaño de muestra y balance en una sola figura."""

plt.figure(figsize=(7, 7))
plt.pie(
    counts.values,
    labels=counts.index,
    autopct="%1.1f%%",
    startangle=90
)
plt.title("Distribucion del target binario (porcentaje)")
plt.tight_layout()
plt.show()

"""Graficamos el porcentaje de cada clase del target binario en un pie chart para una lectura rápida del balance."""

group_gender = (
    df.groupby("Your Gender")["target_bin"]
      .value_counts(normalize=True)
      .mul(100)
      .rename("pct")
      .reset_index()
)

pivot_gender = (
    group_gender.pivot(index="Your Gender", columns="target_bin", values="pct")
               .fillna(0)
)

display(pivot_gender.round(2))

ax = pivot_gender.plot(kind="bar", stacked=True)
ax.set_title("Distribucion del target por genero (porcentaje)")
ax.set_xlabel("Genero")
ax.set_ylabel("Porcentaje (%)")
ax.set_xticklabels(ax.get_xticklabels(), rotation=0)
ax.legend(title="Target", bbox_to_anchor=(1.02, 1), loc="upper left", borderaxespad=0)

plt.tight_layout()
plt.show()

"""Calculamos el porcentaje de REMOTE/FULL_OFFICE dentro de cada género y lo mostramos con barras apiladas. Movemos la leyenda fuera del gráfico para no tapar las barras."""

import matplotlib.pyplot as plt
import pandas as pd

# Asegurar target_bin (por si no existe en memoria)
TARGET_6 = "What is the most preferred working environment for you."
if "target_bin" not in df.columns:
    df["target_bin"] = df[TARGET_6].astype(str).apply(
        lambda x: "FULL_OFFICE" if x.strip() == "Every Day Office Environment" else "REMOTE"
    )

# Abreviar SOLO Estados Unidos
df_plot = df.copy()
df_plot["country_short"] = df_plot["Your Current Country."].replace(
    {"United States of America": "EEUU"}
)

group_country = (
    df_plot.groupby("country_short")["target_bin"]
           .value_counts(normalize=True)
           .mul(100)
           .rename("pct")
           .reset_index()
)

pivot_country = (
    group_country.pivot(index="country_short", columns="target_bin", values="pct")
                .fillna(0)
)

display(pivot_country.round(2))

ax = pivot_country.plot(kind="bar", stacked=True)
ax.set_title("Distribucion del target por pais (porcentaje)")
ax.set_xlabel("Pais")
ax.set_ylabel("Porcentaje (%)")
ax.set_xticklabels(ax.get_xticklabels(), rotation=0)

# Leyenda fuera para que no se superponga
ax.legend(title="Target", bbox_to_anchor=(1.02, 1), loc="upper left", borderaxespad=0)

plt.tight_layout()
plt.show()

"""Creamos una columna country_short donde solo reemplazamos “United States of America” por “EEUU” para mejorar la legibilidad. Luego calculamos porcentajes por país y target binario y graficamos barras apiladas. La leyenda se ubica fuera del área del gráfico para evitar superposición.

La muestra está fuertemente concentrada en India. Por lo tanto, los resultados (distribución del target y patrones por segmento) reflejan principalmente las preferencias de participantes de India y podrían no generalizarse a otros países con pocos registros en el dataset. En el análisis por país, las proporciones de países con baja participación deben interpretarse con cautela.
"""

country_counts = df["Your Current Country."].value_counts()
country_pct = (country_counts / country_counts.sum() * 100).round(2)
pd.DataFrame({"count": country_counts, "pct": country_pct}).head(10)

"""Composición de la muestra por país (representatividad): La muestra está fuertemente concentrada en India: 98.30% de las respuestas (n=231 de 235) provienen de ese país. Los demás países tienen una presencia mínima (Alemania: 0.85% n=2; Emiratos Árabes Unidos: 0.43% n=1; Estados Unidos: 0.43% n=1). Por lo tanto, los resultados del análisis (distribución del target y patrones por segmento) reflejan principalmente preferencias del grupo de India, y cualquier comparación “por país” fuera de India debe interpretarse con cautela debido al tamaño muestral extremadamente bajo en esos países."""

pd.crosstab(df["Your Current Country."], df["target_bin"], margins=True)

"""Decisión de filtrado por representatividad: Dado que el 98.30% de los registros proviene de India (n=231/235) y que los países no-India suman solo 4 observaciones (n=4) —todas pertenecientes a la clase REMOTE—, se optó por restringir el análisis y el entrenamiento del modelo a India. Esto evita que la variable “país” introduzca patrones espurios derivados de tamaños muestrales marginales y mejora la estabilidad e interpretabilidad de las métricas. En consecuencia, los resultados del modelo deben interpretarse como aplicables al contexto representado por la muestra (India)."""

# Filtrar a India (manteniendo el df original intacto)
df_india = df[df["Your Current Country."] == "India"].copy()

df_india.shape, df_india["target_bin"].value_counts()

"""Dataset de trabajo (India): 231 filas

Target binario en India: REMOTE=181, FULL_OFFICE=50

A partir de ahora trabajaremos con df_india como dataset principal.
"""

import matplotlib.pyplot as plt
import pandas as pd

# Columna candidata (tú la tienes en df_india)
COL_SETUP = "Which of the following setup you would like to work ?"

# Copia para plot
df_plot = df_india[[COL_SETUP, "target_bin"]].copy()

# Agrupar categorias raras en OTHER (para que el grafico sea legible)
min_freq = 5
counts_setup = df_plot[COL_SETUP].value_counts()
keep = counts_setup[counts_setup >= min_freq].index
df_plot[COL_SETUP] = df_plot[COL_SETUP].where(df_plot[COL_SETUP].isin(keep), "OTHER")

# Tabla de porcentajes por categoria (100% por fila)
pct_table = (
    pd.crosstab(df_plot[COL_SETUP], df_plot["target_bin"], normalize="index")
      .mul(100)
      .round(2)
)

display(pct_table)

# Plot 100% stacked
ax = pct_table.plot(kind="bar", stacked=True, figsize=(10, 5))

ax.set_title("Target por preferencia de setup (India) - porcentaje")
ax.set_xlabel("Preferencia de setup")
ax.set_ylabel("Porcentaje (%)")
ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha="right")

# Leyenda fuera
ax.legend(title="Target", bbox_to_anchor=(1.02, 1), loc="upper left", borderaxespad=0)

plt.tight_layout()
plt.show()

"""Calculamos la distribución porcentual del target (REMOTE vs FULL_OFFICE) para cada categoría de la variable “setup”. Para mejorar legibilidad, agrupamos categorías con baja frecuencia en “OTHER”. Luego graficamos barras apiladas al 100% para comparar proporciones de manera directa."""

import matplotlib.pyplot as plt
import pandas as pd

counts = df_india["target_bin"].value_counts()
pct = (counts / counts.sum() * 100).round(2)

plt.figure()
ax = counts.plot(kind="bar")
plt.title("Distribucion del target binario (India): conteo y porcentaje")
plt.xlabel("Clase")
plt.ylabel("Cantidad")
plt.xticks(rotation=0)

for i, (label, n) in enumerate(counts.items()):
    ax.text(i, n, f"{n} ({pct[label]}%)", ha="center", va="bottom")

plt.tight_layout()
plt.show()

pd.DataFrame({"count": counts, "pct": pct})

"""Mostramos la distribución del target binario en el dataset filtrado a India, con conteo y porcentaje en un solo gráfico, y una tabla de respaldo."""

plt.figure(figsize=(7, 7))
plt.pie(
    counts.values,
    labels=counts.index,
    autopct="%1.1f%%",
    startangle=90
)
plt.title("Distribucion del target binario (India) - porcentaje")
plt.tight_layout()
plt.show()

"""Visualizamos el balance de clases del target binario en India usando un gráfico de torta."""

group_gender = (
    df_india.groupby("Your Gender")["target_bin"]
            .value_counts(normalize=True)
            .mul(100)
            .rename("pct")
            .reset_index()
)

pivot_gender = (
    group_gender.pivot(index="Your Gender", columns="target_bin", values="pct")
                .fillna(0)
)

display(pivot_gender.round(2))

ax = pivot_gender.plot(kind="bar", stacked=True)
ax.set_title("Distribucion del target por genero (India) - porcentaje")
ax.set_xlabel("Genero")
ax.set_ylabel("Porcentaje (%)")
ax.set_xticklabels(ax.get_xticklabels(), rotation=0)
ax.legend(title="Target", bbox_to_anchor=(1.02, 1), loc="upper left", borderaxespad=0)

plt.tight_layout()
plt.show()

"""Calculamos y graficamos la proporción de REMOTE/FULL_OFFICE por género usando solo India, moviendo la leyenda fuera del gráfico para evitar superposiciones.

En India, el target binario queda: REMOTE 78.35% (n=181) vs FULL_OFFICE 21.65% (n=50).

Por género (porcentajes dentro de cada género):

Female: REMOTE 84.62%, FULL_OFFICE 15.38%

Male: REMOTE 75.16%, FULL_OFFICE 24.84%
"""

import matplotlib.pyplot as plt

# Detectar columnas numericas en df_india (excluyendo zip si existe)
num_cols_all = df_india.select_dtypes(exclude=["object"]).columns.tolist()

# Si el zip esta, lo excluimos porque es identificador
if "Your Current Zip Code / Pin Code" in num_cols_all:
    num_cols_all.remove("Your Current Zip Code / Pin Code")

num_cols_all

"""Identificamos qué columnas numéricas existen en el dataset de India y excluimos el Zip Code porque actúa como identificador (no como variable predictiva continua).

Luego, para cada numérica, hacemos boxplot por target:
"""

import matplotlib.pyplot as plt

col = "How likely would you work for a company whose mission is not bringing social impact ?"

data_office = df_india.loc[df_india["target_bin"] == "FULL_OFFICE", col].dropna()
data_remote = df_india.loc[df_india["target_bin"] == "REMOTE", col].dropna()

plt.figure(figsize=(7, 4))

bp = plt.boxplot(
    [data_office, data_remote],
    tick_labels=["FULL_OFFICE", "REMOTE"],   # evita el warning (matplotlib >= 3.9)
    patch_artist=True,
    showmeans=True
)

# Colores custom (1 por caja)
box_colors = ["#4C78A8", "#F58518"]
for patch, c in zip(bp["boxes"], box_colors):
    patch.set_facecolor(c)
    patch.set_alpha(0.6)

# Estilos extra para claridad
for median in bp["medians"]:
    median.set_linewidth(2)

plt.title("Mission without social impact: score by target (India)")
plt.xlabel("Target class")
plt.ylabel("Likelihood score")
plt.grid(axis="y", linestyle="--", alpha=0.4)

plt.tight_layout()
plt.show()

"""Comparamos la distribución del puntaje numérico entre las clases FULL_OFFICE y REMOTE usando un boxplot. Mejoramos la legibilidad con un título corto, etiquetas claras y una grilla suave. Además, usamos colores personalizados y mostramos la media para facilitar la comparación."""

plt.figure(figsize=(7, 4))

parts = plt.violinplot([data_office, data_remote], showmeans=True, showmedians=True)

violin_colors = ["#4C78A8", "#F58518"]
for body, c in zip(parts["bodies"], violin_colors):
    body.set_facecolor(c)
    body.set_alpha(0.6)

plt.xticks([1, 2], ["FULL_OFFICE", "REMOTE"])
plt.title("Mission without social impact: distribution by target (India)")
plt.xlabel("Target class")
plt.ylabel("Likelihood score")
plt.grid(axis="y", linestyle="--", alpha=0.4)

plt.tight_layout()
plt.show()

"""Usamos un violin plot para visualizar la distribución completa (densidad) del puntaje por clase. Esto es útil para ver no solo la mediana y el rango (como en boxplot), sino también la forma general de la distribución."""

# PASO 4: Definir X e y usando df_india y aplicar DROP_COLS
TARGET_6 = "What is the most preferred working environment for you."

DROP_COLS = [
    "Your Current Zip Code / Pin Code",
    "Which of the below careers looks close to your Aspirational job ?"
]

y = df_india["target_bin"]
X = df_india.drop(columns=[TARGET_6] + DROP_COLS)

X.shape, y.shape

"""Definimos el target y usando la variable binaria target_bin (REMOTE vs FULL_OFFICE). Construimos X eliminando el target original y columnas que pueden introducir ruido (zip como identificador y la columna de carreras por alta cardinalidad). Validamos dimensiones finales."""

# PASO 4: Separar columnas categoricas y numericas
cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
num_cols = X.select_dtypes(exclude=["object"]).columns.tolist()

cat_cols, num_cols

"""Identificamos qué variables son categóricas y cuáles numéricas para aplicar el preprocesamiento correcto (OneHotEncoder para categóricas y tratamiento apropiado para numéricas)."""

# PASO 4: Validaciones rapidas
X_cat_cardinality = X[cat_cols].nunique().sort_values(ascending=False)
X_num_summary = X[num_cols].describe().T

X_cat_cardinality, X_num_summary

"""Revisamos la cardinalidad de las variables categóricas (para justificar la agrupación de categorías raras en "OTHER") y obtenemos estadísticas descriptivas de las variables numéricas para verificar rangos y consistencia.

Problema detectado

En cat_cols aparece target_bin como feature y eso es data leakage (el modelo vería el target dentro de X).
Eso explica también por qué X.shape te quedó (231, 13) en vez de lo esperado (12).

Para continuar correctamente, debemos sacar target_bin de X.
"""

# PASO 4 (corregido): Definir X e y sin leakage
TARGET_6 = "What is the most preferred working environment for you."

DROP_COLS = [
    "Your Current Zip Code / Pin Code",
    "Which of the below careers looks close to your Aspirational job ?",
    "target_bin"  # IMPORTANTE: evitar leakage
]

y = df_india["target_bin"]
X = df_india.drop(columns=[TARGET_6] + DROP_COLS)

X.shape, y.shape

"""Reconstruimos X eliminando explícitamente target_bin para evitar fuga de información (data leakage), ya que target_bin es el objetivo (y) y no debe incluirse como feature. Validamos que las dimensiones queden correctas."""

cat_cols = X.select_dtypes(include=["object"]).columns.tolist()
num_cols = X.select_dtypes(exclude=["object"]).columns.tolist()

cat_cols, num_cols

from sklearn.model_selection import train_test_split

RANDOM_STATE = 42

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.20,
    random_state=RANDOM_STATE,
    stratify=y
)

X_train.shape, X_test.shape, y_train.value_counts(normalize=True).round(3), y_test.value_counts(normalize=True).round(3)

"""Dividimos el dataset en entrenamiento y prueba (80/20) usando estratificación para mantener la proporción de clases REMOTE/FULL_OFFICE en ambos conjuntos. Esto permite evaluar el modelo de forma justa y reproducible."""

def apply_rare_to_other(X_train, X_test, cat_cols, min_freq=2, other_label="OTHER"):
    """
    Aprende en TRAIN qué categorías se conservan por columna (frecuencia >= min_freq)
    y transforma TRAIN y TEST reemplazando categorías raras por other_label.
    Devuelve X_train_new, X_test_new y keep_map (para guardar en API).
    """
    X_train_new = X_train.copy()
    X_test_new = X_test.copy()

    keep_map = {}

    for col in cat_cols:
        vc = X_train_new[col].value_counts(dropna=False)
        keep = vc[vc >= min_freq].index.astype(str).tolist()
        keep_map[col] = keep

        X_train_new[col] = X_train_new[col].astype(str).where(X_train_new[col].astype(str).isin(keep), other_label)
        X_test_new[col] = X_test_new[col].astype(str).where(X_test_new[col].astype(str).isin(keep), other_label)

    return X_train_new, X_test_new, keep_map

X_train_r, X_test_r, keep_map = apply_rare_to_other(
    X_train, X_test, cat_cols, min_freq=2, other_label="OTHER"
)

# Chequeo rapido: cardinalidad post-agrupacion (top 10)
X_train_r[cat_cols].nunique().sort_values(ascending=False).head(10)

"""Reducimos categorías raras agrupándolas como “OTHER”. La regla se aprende solo en TRAIN para evitar fuga de información y se aplica igual a TEST. Guardamos keep_map para reproducir el mismo tratamiento en producción (API).

Split correcto: train (184,12) / test (47,12)

Estratificación OK (REMOTE ~0.78 en ambos).

Rare→OTHER aplicado y cardinalidades razonables (setup bajó a 16, etc.).

"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.dummy import DummyClassifier
from sklearn.linear_model import LogisticRegression

# Preprocesamiento: OHE para categoricas, passthrough para numericas
preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ("num", "passthrough", num_cols)
    ],
    remainder="drop"
)

"""Construimos un preprocesador que aplica One-Hot Encoding a las variables categóricas y mantiene las numéricas sin transformación (passthrough). handle_unknown="ignore" evita errores si aparecen categorías nuevas en test o producción."""

dummy_clf = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", DummyClassifier(strategy="most_frequent", random_state=42))
])

dummy_clf.fit(X_train_r, y_train)

y_pred_dummy = dummy_clf.predict(X_test_r)

acc_dummy = accuracy_score(y_test, y_pred_dummy)
f1_dummy = f1_score(y_test, y_pred_dummy, pos_label="REMOTE")

acc_dummy, f1_dummy

"""Entrenamos un modelo base “ingenuo” que siempre predice la clase más frecuente. Esto sirve como referencia mínima: cualquier modelo útil debe superar este baseline en métricas como Accuracy y F1."""

logreg_clf = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", LogisticRegression(
        max_iter=5000,
        solver="saga",
        penalty="l1",
        C=1.0,
        class_weight="balanced",
        random_state=42
    ))
])

logreg_clf.fit(X_train_r, y_train)

y_pred_lr = logreg_clf.predict(X_test_r)

acc_lr = accuracy_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr, pos_label="REMOTE")

acc_lr, f1_lr

"""Entrenamos una Regresión Logística con regularización L1 (selección de variables) y class_weight="balanced" para compensar el desbalance de clases. Este modelo permite obtener probabilidades (predict_proba), necesarias para reportar confianza en la API."""

print("=== DUMMY (most_frequent) ===")
print(classification_report(y_test, y_pred_dummy, digits=3))

print("\n=== LOGISTIC REGRESSION ===")
print(classification_report(y_test, y_pred_lr, digits=3))

"""Mostramos el classification report con precision, recall y F1 por clase para comparar el baseline versus el modelo base y entender el desempeño más allá de accuracy.

(0.7872, 0.8810) = acc_dummy, f1_dummy (Dummy predice siempre REMOTE).

(0.7234, 0.8060) = acc_lr, f1_lr (LogReg con class_weight balanced).

Los dos bloques son los classification_report de Dummy y LogReg.

El warning es esperable: el Dummy nunca predice FULL_OFFICE, por eso la precisión de esa clase es indefinida.

Insight importante (para tu reporte)

El Dummy tiene accuracy alto porque REMOTE es la clase mayoritaria (≈78%). Pero no sirve: F1 de FULL_OFFICE es 0.

LogReg baja accuracy, pero por fin detecta FULL_OFFICE (recall 0.70). Eso es mucho más valioso si te importa capturar esa clase.

El F1 que se calculo es para la clase positiva REMOTE. Si quieres una métrica más “justa” para ambas clases, usaremos también F1 macro en el tuning (como se hizo en el ejercicio anterior).
"""

from sklearn.metrics import f1_score, roc_auc_score

# Probabilidades para ROC AUC (clase positiva = REMOTE)
y_proba_lr = logreg_clf.predict_proba(X_test_r)[:, 1]
y_true_bin = (y_test == "REMOTE").astype(int)

f1_macro_lr = f1_score(y_test, y_pred_lr, average="macro")
roc_auc_lr = roc_auc_score(y_true_bin, y_proba_lr)

f1_macro_lr, roc_auc_lr

"""Agregamos F1 macro para evaluar desempeño equilibrado entre clases y ROC AUC para medir capacidad discriminativa usando probabilidades. Esto complementa Accuracy y F1 de la clase REMOTE."""

import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix

labels_order = ["FULL_OFFICE", "REMOTE"]
cm = confusion_matrix(y_test, y_pred_lr, labels=labels_order)

fig, ax = plt.subplots(figsize=(6, 5), constrained_layout=True)

disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_order)
disp.plot(ax=ax, cmap="Blues", colorbar=True, values_format="d")

ax.set_title("Matriz de confusion - Logistic Regression (India)", pad=12)
ax.set_xlabel("Predicted label")
ax.set_ylabel("True label")

# Ajuste fino: reduce un poco el espacio ocupado por el colorbar
# (si no se solapa, puedes dejarlo igual)
plt.show()

"""Mostramos la matriz de confusión del modelo, que permite ver aciertos y errores por clase (FULL_OFFICE y REMOTE). Personalizamos el color del mapa (cmap) y el título para cumplir la rúbrica."""

import matplotlib.pyplot as plt
from sklearn.metrics import RocCurveDisplay

fig, ax = plt.subplots(figsize=(6, 5), constrained_layout=True)

RocCurveDisplay.from_predictions(
    y_true_bin,
    y_proba_lr,
    name="LogReg (REMOTE positive)",
    ax=ax
)

ax.set_title("Curva ROC - Logistic Regression (India)", pad=12)
ax.grid(True, linestyle="--", alpha=0.4)

# Mover leyenda fuera para que no tape la curva
ax.legend(loc="lower left", bbox_to_anchor=(0.0, -0.2), ncol=1, frameon=True)

plt.show()

"""Graficamos la curva ROC usando las probabilidades del modelo para la clase REMOTE. Incluimos título, grilla y leyenda para cumplir la rúbrica y facilitar interpretación."""

cm

"""Confirma que los 2 gráficos (conf matrix + ROC) se ven bien."""

from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import make_scorer, f1_score
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression

# Preprocess: OHE en cat + escalado en num (recomendado para LogReg)
preprocess_tune = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ("num", Pipeline(steps=[("scaler", StandardScaler())]), num_cols)
    ],
    remainder="drop"
)

pipe_lr = Pipeline(steps=[
    ("preprocess", preprocess_tune),
    ("model", LogisticRegression(
        solver="saga",
        max_iter=5000,
        class_weight="balanced",
        random_state=42
    ))
])

param_grid = {
    "model__penalty": ["l1", "l2"],
    "model__C": [0.01, 0.1, 1, 10, 50]
}

cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)

# Scorer F1 para clase positiva REMOTE
f1_remote = make_scorer(f1_score, pos_label="REMOTE")

"""Configuramos un pipeline con OneHotEncoder para categóricas y StandardScaler para numéricas, y definimos la regresión logística. Preparamos un grid simple (penalty y C) y una validación cruzada repetida estratificada. Definimos un scorer F1 específico para la clase REMOTE."""

grid_f1_remote = GridSearchCV(
    estimator=pipe_lr,
    param_grid=param_grid,
    scoring=f1_remote,
    cv=cv,
    n_jobs=-1
)

grid_f1_remote.fit(X_train_r, y_train)

grid_f1_remote.best_params_, grid_f1_remote.best_score_

"""Buscamos la mejor combinación de hiperparámetros maximizando F1 para la clase REMOTE mediante validación cruzada."""

grid_f1_macro = GridSearchCV(
    estimator=pipe_lr,
    param_grid=param_grid,
    scoring="f1_macro",
    cv=cv,
    n_jobs=-1
)

grid_f1_macro.fit(X_train_r, y_train)

grid_f1_macro.best_params_, grid_f1_macro.best_score_

"""Repetimos la búsqueda optimizando F1 macro para lograr un desempeño más equilibrado entre FULL_OFFICE y REMOTE."""

from sklearn.metrics import accuracy_score, f1_score, classification_report

best_remote = grid_f1_remote.best_estimator_
best_macro = grid_f1_macro.best_estimator_

pred_remote = best_remote.predict(X_test_r)
pred_macro = best_macro.predict(X_test_r)

results = {
    "best_by_f1_remote": {
        "accuracy": accuracy_score(y_test, pred_remote),
        "f1_remote": f1_score(y_test, pred_remote, pos_label="REMOTE"),
        "f1_macro": f1_score(y_test, pred_remote, average="macro")
    },
    "best_by_f1_macro": {
        "accuracy": accuracy_score(y_test, pred_macro),
        "f1_remote": f1_score(y_test, pred_macro, pos_label="REMOTE"),
        "f1_macro": f1_score(y_test, pred_macro, average="macro")
    }
}

results

"""Comparamos en el conjunto de prueba los dos modelos optimizados con distintos objetivos (F1 REMOTE vs F1 macro) usando accuracy y ambas variantes de F1 para decidir cuál se ajusta mejor a nuestro objetivo.

El tuning encontró un modelo más regularizado (C=0.01) con L2.

Como ambos objetivos eligieron el mismo modelo, no hay conflicto entre optimizar REMOTE vs macro en este rango de búsqueda.

En test, el accuracy bajó respecto al modelo base anterior, pero el F1 macro es razonable. Para decidir bien, falta mirar confusion matrix del modelo tuned (porque FULL_OFFICE es la clase minoritaria y es la que más nos importa “no perder”).
"""

best_lr = grid_f1_remote.best_estimator_  # es igual al best_macro

pred_lr = best_lr.predict(X_test_r)

metrics_lr = {
    "model": "LogReg (tuned)",
    "accuracy": accuracy_score(y_test, pred_lr),
    "f1_remote": f1_score(y_test, pred_lr, pos_label="REMOTE"),
    "f1_macro": f1_score(y_test, pred_lr, average="macro"),
}
metrics_lr

"""Tomamos el mejor modelo encontrado por GridSearchCV (tuned) y calculamos métricas en test para usarlo como baseline fuerte contra el ensamble."""

from sklearn.ensemble import RandomForestClassifier

preprocess_rf = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ("num", "passthrough", num_cols)
    ],
    remainder="drop"
)

rf_clf = Pipeline(steps=[
    ("preprocess", preprocess_rf),
    ("model", RandomForestClassifier(
        n_estimators=400,
        max_depth=None,
        min_samples_split=2,
        min_samples_leaf=1,
        class_weight="balanced",
        random_state=42,
        n_jobs=-1
    ))
])

rf_clf.fit(X_train_r, y_train)
pred_rf = rf_clf.predict(X_test_r)

metrics_rf = {
    "model": "RandomForest (baseline)",
    "accuracy": accuracy_score(y_test, pred_rf),
    "f1_remote": f1_score(y_test, pred_rf, pos_label="REMOTE"),
    "f1_macro": f1_score(y_test, pred_rf, average="macro"),
}
metrics_rf

"""Entrenamos un RandomForest (ensamble de bagging) con balanceo de clases y el mismo preprocesamiento (OHE + num passthrough). Luego evaluamos en test con las mismas métricas para comparar directamente."""

import pandas as pd

df_compare = pd.DataFrame([metrics_lr, metrics_rf]).set_index("model")
df_compare

"""Creamos una tabla comparativa con métricas en test para elegir el modelo con mejor balance entre clases (priorizando F1 macro y capacidad de predecir FULL_OFFICE)."""

from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
import matplotlib.pyplot as plt

labels_order = ["FULL_OFFICE", "REMOTE"]

def plot_cm(y_true, y_pred, title):
    cm = confusion_matrix(y_true, y_pred, labels=labels_order)
    fig, ax = plt.subplots(figsize=(6, 5), constrained_layout=True)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_order)
    disp.plot(ax=ax, cmap="Blues", colorbar=True, values_format="d")
    ax.set_title(title, pad=12)
    ax.set_xlabel("Predicted label")
    ax.set_ylabel("True label")
    plt.show()
    return cm

cm_lr = plot_cm(y_test, pred_lr, "Matriz de confusion - LogReg tuned (India)")
cm_rf = plot_cm(y_test, pred_rf, "Matriz de confusion - RandomForest (India)")

cm_lr, cm_rf

"""Graficamos matrices de confusión para ambos modelos con una paleta de colores personalizada, para ver el rendimiento por clase (especialmente FULL_OFFICE). Esto cumple con la rúbrica de gráficas de rendimiento.

Matrices de confusión

LogReg tuned [[6,4],[10,27]]

FULL_OFFICE (soporte=10): recall = 6/10 = 0.60

REMOTE (soporte=37): recall = 27/37 ≈ 0.73

RandomForest [[1,9],[2,35]]

FULL_OFFICE: recall = 1/10 = 0.10 (muy malo)

REMOTE: recall = 35/37 ≈ 0.95

Métricas

RF tiene mejor accuracy y F1_remote, pero F1_macro peor (0.509) porque “abandona” FULL_OFFICE.

LogReg tuned tiene menor accuracy, pero mejor equilibrio (F1_macro 0.628) y captura mucho más FULL_OFFICE.
"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
import pandas as pd

# Preprocess: igual que RF (sin scaler obligatorio)
preprocess_gb = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ("num", "passthrough", num_cols)
    ],
    remainder="drop"
)

gb_clf = Pipeline(steps=[
    ("preprocess", preprocess_gb),
    ("model", GradientBoostingClassifier(random_state=42))
])

gb_clf.fit(X_train_r, y_train)
pred_gb = gb_clf.predict(X_test_r)

metrics_gb = {
    "model": "GradientBoosting (baseline)",
    "accuracy": accuracy_score(y_test, pred_gb),
    "f1_remote": f1_score(y_test, pred_gb, pos_label="REMOTE"),
    "f1_macro": f1_score(y_test, pred_gb, average="macro"),
}
metrics_gb

"""Entrenamos un modelo de boosting (GradientBoostingClassifier) con el mismo preprocesamiento (OHE + num passthrough) y evaluamos en test con las mismas métricas para compararlo contra LogReg tuned."""

df_compare2 = pd.DataFrame([metrics_lr, metrics_gb]).set_index("model")
df_compare2

"""Creamos una tabla comparativa con métricas en test para ver si el boosting mejora el equilibrio entre clases (F1 macro) sin sacrificar demasiado accuracy."""

from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt

labels_order = ["FULL_OFFICE", "REMOTE"]

cm_gb = confusion_matrix(y_test, pred_gb, labels=labels_order)

fig, ax = plt.subplots(figsize=(6, 5), constrained_layout=True)
disp = ConfusionMatrixDisplay(confusion_matrix=cm_gb, display_labels=labels_order)
disp.plot(ax=ax, cmap="Purples", colorbar=True, values_format="d")  # color custom distinto
ax.set_title("Matriz de confusion - GradientBoosting (India)", pad=12)
ax.set_xlabel("Predicted label")
ax.set_ylabel("True label")
plt.show()

cm_gb

"""Graficamos la matriz de confusión del boosting con un color map personalizado (Purples) para cumplir la rúbrica y analizar el desempeño por clase, especialmente FULL_OFFICE.

Lectura honesta de Boosting
Matriz de confusión GB [[1,9],[4,33]]

FULL_OFFICE (10): recall = 1/10 = 0.10 → prácticamente no detecta FULL_OFFICE.

REMOTE (37): recall = 33/37 ≈ 0.892

Métricas

Accuracy: 0.723 (mejor que LogReg tuned 0.702)

F1_remote: 0.835 (mejor que 0.794)

F1_macro: 0.484 (mucho peor que 0.628)

Conclusión: Boosting (baseline) se comporta igual que RF: “se va” a REMOTE y abandona la clase minoritaria. Para tu proyecto, no es buen modelo final si quieres un desempeño equilibrado y defendible.

Modelo final sigue siendo LogReg tuned por F1_macro y por recall de FULL_OFFICE (0.60 vs 0.10).
"""

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, make_scorer

# Pipeline con scaler en numéricas
preprocess_en = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
        ("num", Pipeline(steps=[("scaler", StandardScaler())]), num_cols)
    ],
    remainder="drop"
)

pipe_en = Pipeline(steps=[
    ("preprocess", preprocess_en),
    ("model", LogisticRegression(
        solver="saga",
        max_iter=5000,
        class_weight="balanced",
        random_state=42
    ))
])

param_grid_en = {
    "model__penalty": ["elasticnet"],
    "model__C": [0.001, 0.01, 0.1, 1, 10],
    "model__l1_ratio": [0.1, 0.5, 0.9]
}

grid_en = GridSearchCV(
    estimator=pipe_en,
    param_grid=param_grid_en,
    scoring="f1_macro",
    cv=cv,
    n_jobs=-1
)

grid_en.fit(X_train_r, y_train)

grid_en.best_params_, grid_en.best_score_

"""Entrenamos una regresión logística con elastic-net (combinación L1/L2) y buscamos hiperparámetros que maximicen F1 macro mediante validación cruzada repetida estratificada, priorizando equilibrio entre FULL_OFFICE y REMOTE."""

best_en = grid_en.best_estimator_

pred_en = best_en.predict(X_test_r)

metrics_en = {
    "model": "LogReg (elastic-net)",
    "accuracy": accuracy_score(y_test, pred_en),
    "f1_remote": f1_score(y_test, pred_en, pos_label="REMOTE"),
    "f1_macro": f1_score(y_test, pred_en, average="macro"),
}
metrics_en

"""Evaluamos el mejor modelo elastic-net en el conjunto de prueba con las mismas métricas, para compararlo contra LogReg tuned (L2) y decidir si mejora el balance."""

import numpy as np
import pandas as pd

proba_remote = best_en.predict_proba(X_test_r)[:, 1]
y_true = y_test.values

thresholds = np.linspace(0.1, 0.9, 17)

rows = []
for t in thresholds:
    y_pred_t = np.where(proba_remote >= t, "REMOTE", "FULL_OFFICE")
    rows.append({
        "threshold": float(np.round(t, 2)),
        "accuracy": accuracy_score(y_true, y_pred_t),
        "f1_macro": f1_score(y_true, y_pred_t, average="macro"),
        "f1_remote": f1_score(y_true, y_pred_t, pos_label="REMOTE"),
        "f1_full_office": f1_score(y_true, y_pred_t, pos_label="FULL_OFFICE"),
    })

df_thresh = pd.DataFrame(rows).sort_values("f1_macro", ascending=False)
df_thresh.head(10)

"""Probamos distintos umbrales para convertir probabilidades en clases. Elegimos el umbral que maximiza F1 macro (balance entre clases) y reportamos también F1 por clase para ver explícitamente cuánto mejora FULL_OFFICE.

Lectura de lo que pasó
1) Elastic-net como modelo (umbral 0.50)

CV F1_macro ≈ 0.6026 (peor que tu LogReg tuned L2, que ya vimos con macro F1 test ≈ 0.6278)

En test con 0.50:

accuracy 0.6809

f1_remote 0.7761

f1_macro 0.6103
➡️ Elastic-net no mejoró el modelo final (ni en CV ni en test).

2) Ajuste de umbral (esto SÍ es interesante)

Tu tabla muestra que con threshold = 0.45:

accuracy = 0.7447

f1_macro = 0.6439

f1_remote = 0.8333

f1_full_office = 0.4545

Comparado con tu LogReg tuned actual (threshold 0.50):

accuracy ≈ 0.7021

f1_macro ≈ 0.6278

f1_remote ≈ 0.7941

➡️ Con t=0.45 lograste mejorar accuracy y F1_macro a la vez (al menos en ese test). Eso es una mejora real en métrica.

Pero ojo metodológico importante (para ser 100% honesto):
Ese umbral fue elegido mirando el test, así que no lo podemos vender como “generaliza mejor” con certeza. Para hacerlo correcto y defendible, el umbral se debe elegir con validación (por ejemplo, usando CV o un validation split dentro de train) y luego evaluar una sola vez en test.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import (
    accuracy_score, balanced_accuracy_score,
    precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score,
    classification_report, confusion_matrix,
    ConfusionMatrixDisplay, RocCurveDisplay, PrecisionRecallDisplay
)

# Modelo final (tuned)
best_model = grid_f1_remote.best_estimator_  # o reemplaza por: best_model = best_lr

labels_order = ["FULL_OFFICE", "REMOTE"]

# Predicciones
y_pred = best_model.predict(X_test_r)
y_proba_remote = best_model.predict_proba(X_test_r)[:, 1]  # proba de REMOTE

# Métricas globales
metrics_global = {
    "accuracy": accuracy_score(y_test, y_pred),
    "balanced_accuracy": balanced_accuracy_score(y_test, y_pred),
    "f1_macro": f1_score(y_test, y_pred, average="macro"),
    "f1_weighted": f1_score(y_test, y_pred, average="weighted"),
    "f1_remote": f1_score(y_test, y_pred, pos_label="REMOTE"),
    "f1_full_office": f1_score(y_test, y_pred, pos_label="FULL_OFFICE"),
    "roc_auc_remote": roc_auc_score((y_test == "REMOTE").astype(int), y_proba_remote),
    "avg_precision_remote": average_precision_score((y_test == "REMOTE").astype(int), y_proba_remote),
}

pd.Series(metrics_global).sort_index()

"""Calculamos métricas globales del modelo final: accuracy, balanced accuracy, F1 macro/weighted y F1 por clase. Además, calculamos ROC AUC y Average Precision (área bajo curva Precision-Recall) tomando REMOTE como clase positiva, usando predict_proba."""

print("=== Classification report (por clase) ===")
print(classification_report(y_test, y_pred, labels=labels_order, digits=4, zero_division=0))

"""Mostramos el classification report completo con precision, recall, f1-score y support por clase. zero_division=0 evita warnings cuando alguna clase no es predicha."""

cm = confusion_matrix(y_test, y_pred, labels=labels_order)

fig, ax = plt.subplots(figsize=(6, 5), constrained_layout=True)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_order)
disp.plot(ax=ax, cmap="Blues", colorbar=True, values_format="d")
ax.set_title("Matriz de confusion - Modelo final (LogReg tuned)", pad=12)
ax.set_xlabel("Predicted label")
ax.set_ylabel("True label")
plt.show()

cm

"""Graficamos la matriz de confusión del modelo final con colores personalizados. Esto permite ver exactamente cuántos FULL_OFFICE y REMOTE se clasifican bien o se confunden."""

# ROC
fig, ax = plt.subplots(figsize=(6, 5), constrained_layout=True)
RocCurveDisplay.from_predictions(
    (y_test == "REMOTE").astype(int),
    y_proba_remote,
    ax=ax,
    name=f"LogReg (AUC={metrics_global['roc_auc_remote']:.2f})"
)
ax.set_title("Curva ROC - Modelo final (REMOTE positiva)", pad=12)
ax.grid(True, linestyle="--", alpha=0.4)
ax.legend(loc="lower right")
plt.show()

# Precision-Recall
fig, ax = plt.subplots(figsize=(6, 5), constrained_layout=True)
PrecisionRecallDisplay.from_predictions(
    (y_test == "REMOTE").astype(int),
    y_proba_remote,
    ax=ax,
    name=f"LogReg (AP={metrics_global['avg_precision_remote']:.2f})"
)
ax.set_title("Curva Precision-Recall - Modelo final (REMOTE positiva)", pad=12)
ax.grid(True, linestyle="--", alpha=0.4)
ax.legend(loc="lower left")
plt.show()

"""Graficamos ROC y Precision-Recall, que son útiles cuando hay desbalance. ROC AUC mide separabilidad general; Average Precision (AP) resume la curva PR y suele ser más informativa con clases minoritarias."""

thresholds = np.linspace(0.1, 0.9, 17)

rows = []
for t in thresholds:
    y_pred_t = np.where(y_proba_remote >= t, "REMOTE", "FULL_OFFICE")
    rows.append({
        "threshold": float(np.round(t, 2)),
        "accuracy": accuracy_score(y_test, y_pred_t),
        "f1_macro": f1_score(y_test, y_pred_t, average="macro"),
        "recall_full_office": recall_score(y_test, y_pred_t, pos_label="FULL_OFFICE"),
        "recall_remote": recall_score(y_test, y_pred_t, pos_label="REMOTE"),
    })

df_tradeoff = pd.DataFrame(rows).sort_values("f1_macro", ascending=False)
df_tradeoff.head(10)

"""Exploramos cómo cambia el rendimiento si movemos el umbral que decide REMOTE vs FULL_OFFICE usando probabilidades. Esto ayuda a explicar “por qué” la API devuelve cierta predicción y cómo se relaciona la confianza con el umbral."""

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score

# Usamos el modelo tuned ya validado (L2, C=0.01) desde grid_f1_remote
base_model = grid_f1_remote.best_estimator_

thresholds = np.linspace(0.1, 0.9, 17)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

rows = []

X_train_arr = X_train_r.copy()
y_train_arr = y_train.copy()

for t in thresholds:
    f1s = []
    for tr_idx, va_idx in skf.split(X_train_arr, y_train_arr):
        X_tr = X_train_arr.iloc[tr_idx]
        y_tr = y_train_arr.iloc[tr_idx]
        X_va = X_train_arr.iloc[va_idx]
        y_va = y_train_arr.iloc[va_idx]

        base_model.fit(X_tr, y_tr)
        proba_va = base_model.predict_proba(X_va)[:, 1]

        y_pred_va = np.where(proba_va >= t, "REMOTE", "FULL_OFFICE")
        f1s.append(f1_score(y_va, y_pred_va, average="macro"))

    rows.append({"threshold": float(np.round(t, 2)), "cv_f1_macro_mean": float(np.mean(f1s))})

df_thr_cv = pd.DataFrame(rows).sort_values("cv_f1_macro_mean", ascending=False)
df_thr_cv.head(10)

"""Seleccionamos el umbral de clasificación (probabilidad mínima para predecir REMOTE) usando validación cruzada dentro del conjunto de entrenamiento. Esto evita ajustar el umbral mirando el test y hace la selección metodológicamente correcta. Elegimos el umbral que maximiza F1 macro promedio."""

best_t = df_thr_cv.iloc[0]["threshold"]

# Re-entrenar en todo TRAIN
base_model.fit(X_train_r, y_train)

proba_test = base_model.predict_proba(X_test_r)[:, 1]
pred_test_t = np.where(proba_test >= best_t, "REMOTE", "FULL_OFFICE")

metrics_thr = {
    "threshold": best_t,
    "accuracy": accuracy_score(y_test, pred_test_t),
    "f1_remote": f1_score(y_test, pred_test_t, pos_label="REMOTE"),
    "f1_macro": f1_score(y_test, pred_test_t, average="macro"),
    "f1_full_office": f1_score(y_test, pred_test_t, pos_label="FULL_OFFICE"),
}
metrics_thr

"""Evaluamos en el conjunto de prueba el modelo tuned usando el umbral seleccionado por validación interna. Reportamos accuracy y F1 macro, y también F1 por clase para verificar mejoras en FULL_OFFICE sin perder demasiado REMOTE.

La “mejora” que vimos con threshold=0.45 fue un efecto de mirar el test; cuando lo hacemos correctamente (seleccionando umbral con CV), el mejor umbral es 0.50, o sea no hay mejora real defendible por umbral.

Por tanto, el mejor modelo final defendible sigue siendo:
Logistic Regression tuned (L2, C=0.01) con umbral 0.50
y tu ensamble (RF/GB) queda como comparación para rúbrica.
"""

import joblib
from sklearn.preprocessing import StandardScaler

MIN_FREQ = 2
OTHER_LABEL = "OTHER"
FINAL_THRESHOLD = 0.50

# Aplicar rare->OTHER aprendiendo sobre TODO el dataset (para produccion)
X_all_r, _, keep_map_all = apply_rare_to_other(
    X, X, cat_cols, min_freq=MIN_FREQ, other_label=OTHER_LABEL
)

final_pipeline = Pipeline(steps=[
    ("preprocess", ColumnTransformer(
        transformers=[
            ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
            ("num", Pipeline(steps=[("scaler", StandardScaler())]), num_cols)
        ],
        remainder="drop"
    )),
    ("model", LogisticRegression(
        solver="saga",
        max_iter=5000,
        class_weight="balanced",
        random_state=42,
        penalty="l2",
        C=0.01
    ))
])

final_pipeline.fit(X_all_r, y)

final_pipeline

"""Reentrenamos el modelo final usando todo el dataset de India para maximizar la información disponible en producción. Además, aprendemos la regla de categorías raras (rare→OTHER) usando todos los datos y la guardamos para inferencia consistente en la API."""

artifact = {
    "model": final_pipeline,
    "labels": ["FULL_OFFICE", "REMOTE"],
    "cat_cols": cat_cols,
    "num_cols": num_cols,
    "min_freq": MIN_FREQ,
    "other_label": OTHER_LABEL,
    "keep_map": keep_map_all,
    "threshold": FINAL_THRESHOLD,
    "model_version": "logreg_l2_c0p01_v1"
}

artifact_path = "model_logreg_fulloffice_remote_v1.joblib"
joblib.dump(artifact, artifact_path)

artifact_path

"""Guardamos en un archivo joblib el pipeline entrenado y toda la metadata necesaria para reproducir el preprocesamiento y la inferencia en producción, incluyendo el umbral de decisión y la versión del modelo."""

import os
os.path.getsize(artifact_path)

!pip -q install flask pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from flask import Flask, request, jsonify
# import pandas as pd
# import joblib
# 
# ARTIFACT_PATH = "model_logreg_fulloffice_remote_v1.joblib"
# 
# artifact = joblib.load(ARTIFACT_PATH)
# model = artifact["model"]
# keep_map = artifact["keep_map"]
# other_label = artifact["other_label"]
# threshold = float(artifact["threshold"])
# model_version = artifact["model_version"]
# 
# app = Flask(__name__)
# 
# # Mapeo de nombres simples -> columnas reales del dataset
# COLMAP = {
#     "Your_Current_Country": "Your Current Country.",
#     "Your_Gender": "Your Gender",
#     "Factors_influencing_career_aspirations": "Which of the below factors influence the most about your career aspirations ?",
#     "Higher_Education_outside_India_self_sponsor": "Would you definitely pursue a Higher Education / Post Graduation outside of India ? If only you have to self sponsor it.",
#     "Likely_work_for_one_employer_3_years": "How likely is that you will work for one employer for 3 years or more ?",
#     "Work_for_company_mission_not_defined": "Would you work for a company whose mission is not clearly defined and publicly posted.",
#     "Work_for_company_mission_misaligned": "How likely would you work for a company whose mission is misaligned with their public actions or even their product ?",
#     "Employers_you_would_work_with": "Which of the below Employers would you work with.",
#     "Learning_environment": "Which type of learning environment that you are most likely to work in ?",
#     "Manager_type": "What type of Manager would you work without looking into your watch ?",
#     "Preferred_setup": "Which of the following setup you would like to work ?",
#     "Likely_work_for_company_no_social_impact": "How likely would you work for a company whose mission is not bringing social impact ?"
# }
# 
# REQUIRED_FIELDS = list(COLMAP.keys())
# 
# def apply_keep_map_to_other(df: pd.DataFrame, keep_map: dict, other_label: str) -> pd.DataFrame:
#     df2 = df.copy()
#     for col, allowed in keep_map.items():
#         if col in df2.columns:
#             allowed_set = set(allowed)
#             df2[col] = df2[col].where(df2[col].isin(allowed_set), other_label)
#     return df2
# 
# @app.route("/", methods=["GET"])
# def home():
#     return jsonify({
#         "status": "ok",
#         "model_version": model_version,
#         "threshold": threshold,
#         "endpoints": ["/predict"]
#     })
# 
# @app.route("/predict", methods=["POST"])
# def predict():
#     payload = request.get_json(silent=True)
# 
#     if payload is None:
#         return jsonify({"error": "Invalid or missing JSON body"}), 400
# 
#     missing = [k for k in REQUIRED_FIELDS if k not in payload]
#     if missing:
#         return jsonify({"error": "Missing required fields", "missing": missing}), 400
# 
#     # Construir fila con columnas reales
#     data = {COLMAP[k]: payload[k] for k in REQUIRED_FIELDS}
#     df_in = pd.DataFrame([data])
# 
#     # Aplicar rare -> OTHER
#     df_in = apply_keep_map_to_other(df_in, keep_map, other_label)
# 
#     # Predicción y probabilidad
#     proba_remote = float(model.predict_proba(df_in)[:, 1][0])  # prob(Remote)
#     pred = "REMOTE" if proba_remote >= threshold else "FULL_OFFICE"
#     confidence = proba_remote if pred == "REMOTE" else (1.0 - proba_remote)
# 
#     return jsonify({
#         "prediction": pred,
#         "confidence": confidence,
#         "model_version": model_version,
#         "threshold": threshold,
#         "notes": "confidence derived from predict_proba and threshold"
#     })
# 
# if __name__ == "__main__":
#     # Importante: host 0.0.0.0 para Colab
#     app.run(host="0.0.0.0", port=8000)
#

# === ARRANQUE LIMPIO NGROK + FLASK (ejecutar tras cualquier restart) ===

from google.colab import userdata
from pyngrok import ngrok, conf
import os, signal, subprocess

# 1) Cargar authtoken desde Secrets (debe llamarse exactamente NGROK_AUTHTOKEN)
token = userdata.get("NGROK_AUTHTOKEN")
conf.get_default().auth_token = token

# 2) Cerrar tuneles ngrok previos (si existieran)
try:
    ngrok.kill()
except Exception:
    pass

# 3) Liberar puerto 8000 si quedo un proceso colgado
try:
    pid = subprocess.check_output(["bash", "-lc", "lsof -t -i:8000 || true"]).decode().strip()
    if pid:
        os.kill(int(pid), signal.SIGKILL)
except Exception:
    pass

print("OK: token cargado, tuneles cerrados, puerto 8000 libre")

!lsof -i :8000

import threading
from pyngrok import ngrok, conf
from google.colab import userdata

conf.get_default().auth_token = userdata.get("NGROK_AUTHTOKEN")

try:
    ngrok.kill()
except:
    pass

public_url = ngrok.connect(8000).public_url
print("Public URL:", public_url)

import app as myapp
threading.Thread(
    target=myapp.app.run,
    kwargs={"host": "0.0.0.0", "port": 8000, "use_reloader": False},
    daemon=True
).start()

print("Flask server started.")
print("Abre esto en el navegador:", public_url + "/")

"""# FIN"""

import pandas as pd

DATA_PATH = "/content/drive/MyDrive/Datasets Bootcamp UDD/Your Career Aspirations of GenZ.csv"

df = pd.read_csv(DATA_PATH)

print("df shape:", df.shape)
print("Primeras columnas:", df.columns.tolist()[:10])

import joblib
import pandas as pd
import numpy as np
import requests

# --- CONFIG ---
ARTIFACT_PATH = "model_logreg_fulloffice_remote_v1.joblib"
PUBLIC_URL = "https://nonbeneficial-uninverted-roseanna.ngrok-free.dev"

TARGET_6 = "What is the most preferred working environment for you."
DROP_COLS = [
    "Your Current Zip Code / Pin Code",
    "Which of the below careers looks close to your Aspirational job ?"
]

# Mapeo API keys -> columnas reales del dataset
COLMAP = {
    "Your_Current_Country": "Your Current Country.",
    "Your_Gender": "Your Gender",
    "Factors_influencing_career_aspirations": "Which of the below factors influence the most about your career aspirations ?",
    "Higher_Education_outside_India_self_sponsor": "Would you definitely pursue a Higher Education / Post Graduation outside of India ? If only you have to self sponsor it.",
    "Likely_work_for_one_employer_3_years": "How likely is that you will work for one employer for 3 years or more ?",
    "Work_for_company_mission_not_defined": "Would you work for a company whose mission is not clearly defined and publicly posted.",
    "Work_for_company_mission_misaligned": "How likely would you work for a company whose mission is misaligned with their public actions or even their product ?",
    "Employers_you_would_work_with": "Which of the below Employers would you work with.",
    "Learning_environment": "Which type of learning environment that you are most likely to work in ?",
    "Manager_type": "What type of Manager would you work without looking into your watch ?",
    "Preferred_setup": "Which of the following setup you would like to work ?",
    "Likely_work_for_company_no_social_impact": "How likely would you work for a company whose mission is not bringing social impact ?"
}

# --- LOAD ARTIFACT ---
artifact = joblib.load(ARTIFACT_PATH)
model = artifact["model"]
keep_map = artifact["keep_map"]
other_label = artifact["other_label"]

# --- HELPERS ---
def apply_keep_map_to_other(df_in: pd.DataFrame, keep_map: dict, other_label: str) -> pd.DataFrame:
    df2 = df_in.copy()
    for col, allowed in keep_map.items():
        if col in df2.columns:
            allowed_set = set(allowed)
            df2[col] = df2[col].where(df2[col].isin(allowed_set), other_label)
    return df2

def json_safe(v):
    # Convierte tipos numpy (int64/float64) a tipos nativos para JSON
    if isinstance(v, (np.integer,)):
        return int(v)
    if isinstance(v, (np.floating,)):
        return float(v)
    return v

def row_to_payload(X_df: pd.DataFrame, idx: int) -> dict:
    row = X_df.iloc[idx]
    payload = {api_key: json_safe(row[col_name]) for api_key, col_name in COLMAP.items()}
    return payload

# --- BUILD X (12 features) ---
X = df.drop(columns=[TARGET_6] + DROP_COLS).copy()
X = apply_keep_map_to_other(X, keep_map, other_label)

# --- PROBAS ---
proba_remote = model.predict_proba(X)[:, 1]
conf_office = 1 - proba_remote

# --- TOP 5 BY CLASS ---
top5_remote_idx = np.argsort(-proba_remote)[:5]
top5_remote_conf = proba_remote[top5_remote_idx]

top5_office_idx = np.argsort(-conf_office)[:5]
top5_office_conf = conf_office[top5_office_idx]

print("TOP 5 REMOTE (idx, P(REMOTE)):")
for i, idx in enumerate(top5_remote_idx, 1):
    print(f"{i}) idx={int(idx)}  P(REMOTE)={float(top5_remote_conf[i-1]):.4f}")

print("\nTOP 5 FULL_OFFICE (idx, CONF=1-P(REMOTE)):")
for i, idx in enumerate(top5_office_idx, 1):
    print(f"{i}) idx={int(idx)}  CONF(FULL_OFFICE)={float(top5_office_conf[i-1]):.4f}")

best_remote_idx = int(top5_remote_idx[0])
best_office_idx = int(top5_office_idx[0])

# --- PAYLOADS (best of top 5) ---
payload_remote_high = row_to_payload(X, best_remote_idx)
payload_fulloffice_high = row_to_payload(X, best_office_idx)

# --- CALL API ---
r1 = requests.post(f"{PUBLIC_URL}/predict", json=payload_remote_high, timeout=30)
r2 = requests.post(f"{PUBLIC_URL}/predict", json=payload_fulloffice_high, timeout=30)

print("\nREMOTE best-of-top5 payload:\n", payload_remote_high)
print("REMOTE response:\n", r1.status_code, r1.json())

print("\nFULL_OFFICE best-of-top5 payload:\n", payload_fulloffice_high)
print("FULL_OFFICE response:\n", r2.status_code, r2.json())

"""Este bloque carga el modelo entrenado (joblib), reconstruye las 12 features, aplica la regla OTHER usando keep_map, calcula probabilidades con predict_proba, selecciona los Top 5 casos con mayor probabilidad de REMOTE y FULL_OFFICE, genera payloads JSON válidos (convirtiendo tipos numpy a tipos nativos) y prueba ambos ejemplos llamando a tu API pública /predict."""

from pyngrok import ngrok

# Cierra tuneles abiertos (por si quedaron)
try:
    ngrok.kill()
except Exception:
    pass

# Mata proceso que use 8000 (si existe)
!lsof -t -i:8000 | xargs -r kill -9
!lsof -i :8000

"""Validación: el último lsof debería salir vacío."""

import threading
import app as myapp

threading.Thread(
    target=myapp.app.run,
    kwargs={"host":"0.0.0.0", "port":8000, "use_reloader":False},
    daemon=True
).start()

print("Flask server started.")

from pyngrok import ngrok
import requests

# Abrir tunel
public_url = ngrok.connect(8000)
PUBLIC_URL = public_url.public_url
print("Public URL:", PUBLIC_URL)

# Test GET /
r = requests.get(f"{PUBLIC_URL}/", timeout=30)
print("GET / status:", r.status_code)
print("GET / json:", r.json())

# Test POST /predict
payload = {
  "Your_Current_Country": "India",
  "Your_Gender": "Male",
  "Factors_influencing_career_aspirations": "People who have changed the world for better",
  "Higher_Education_outside_India_self_sponsor": "Yes, I will earn and do that",
  "Likely_work_for_one_employer_3_years": "Will work for 3 years or more",
  "Work_for_company_mission_not_defined": "No",
  "Work_for_company_mission_misaligned": "Will NOT work for them",
  "Employers_you_would_work_with": "Employer who rewards learning and enables that...",
  "Learning_environment": "Instructor or Expert Learning Programs, Trial ...",
  "Manager_type": "Manager who explains what is expected, sets a ...",
  "Preferred_setup": "Work alone",
  "Likely_work_for_company_no_social_impact": 5
}

rp = requests.post(f"{PUBLIC_URL}/predict", json=payload, timeout=30)
print("POST /predict status:", rp.status_code)
print("POST /predict json:", rp.json())

from google.colab import userdata

t = userdata.get("NGROK_AUTHTOKEN")
print("Token cargado:", t is not None)
print("Largo aprox:", len(t) if t else None)
print("Empieza con:", t[:6] if t else None)